import re
from bs4 import BeautifulSoup
import logging
import requests
import time
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

def scrape_prices_simple(url, original_currency_code=None):
    """
    ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ ê°€ê²© ìŠ¤í¬ë˜í•‘ - ì´ë¯¸ì§€ ì²˜ë¦¬ ì—†ìŒ
    Returns a list of dictionaries containing price and context information
    original_currency_code: ì›ë³¸ URLì˜ í†µí™” ì½”ë“œ (ì˜ˆ: USD, KRW, THB)
    """
    try:
        # ìµœì í™”ëœ Selenium ì‚¬ìš© - AgodaëŠ” JavaScript ì‹¤í–‰ í•„ìš”
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ì°¨ë‹¨
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-web-security')  # ì›¹ ë³´ì•ˆ í•´ì œë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')  # ë Œë”ë§ ìµœì í™”
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-features=TranslateUI')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-features=NetworkService')  # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
        chrome_options.add_argument('--disable-ipc-flooding-protection')  # IPC ìµœì í™”
        chrome_options.add_argument('--window-size=320,240')  # ë”ë”ìš± ì‘ì€ ì°½  
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--log-level=3')
        chrome_options.add_argument('--silent')
        chrome_options.add_argument('--no-zygote')  # í”„ë¡œì„¸ìŠ¤ ìµœì í™”
        chrome_options.add_argument('--single-process')  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # ë´‡ ê°ì§€ ìš°íšŒ
        
        # ë´‡ íƒì§€ ìš°íšŒ (ê°„ë‹¨í•˜ê²Œ)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ìŠ¤í¬ë˜í•‘ ì‚¬ìš© URL: {url}")
        
        start_time = time.time()
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(6)  # 6ì´ˆë¡œ ì ë‹¹íˆ ë‹¨ì¶•
        
        # ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸ (ë¹ ë¥´ê²Œ)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # í˜ì´ì§€ ë¡œë”© ì‹œì‘
            driver.get(url)
            
            # ğŸ’¡ 10KB ì œí•œ: í˜ì´ì§€ ì†ŒìŠ¤ê°€ 10KB ë„˜ìœ¼ë©´ ë°”ë¡œ ì¤‘ë‹¨
            max_wait = 10  # ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
            start_wait = time.time()
            
            while time.time() - start_wait < max_wait:
                current_source = driver.page_source
                if len(current_source.encode('utf-8')) >= 10 * 1024:  # 10KB
                    print("ğŸ“ 10KB ë„ë‹¬ - ë¡œë”© ì¤‘ë‹¨")
                    break
                time.sleep(0.2)  # 0.2ì´ˆë§ˆë‹¤ ì²´í¬
            
            page_source = driver.page_source
            
        except:
            # íƒ€ì„ì•„ì›ƒë˜ì–´ë„ í˜„ì¬ê¹Œì§€ ë¡œë”©ëœ ì†ŒìŠ¤ë¼ë„ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
        finally:
            driver.quit()
        
        load_time = time.time() - start_time
        print(f"âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(page_source, 'html.parser')
        all_text = soup.get_text()
        
        print(f"í˜ì´ì§€ í¬ê¸°: {len(all_text)} ê¸€ì, {len(all_text.encode('utf-8'))} bytes")
        
        # ğŸ¯ ë‹¨ìˆœí™”: "ì‹œì‘ê°€" ë˜ëŠ” "Start Price"ë§Œ ì°¾ê¸°
        patterns = [
            r'ì‹œì‘ê°€\s*â‚©\s*(\d{1,3}(?:,\d{3})+)',  # í•œêµ­ì–´ "ì‹œì‘ê°€ â‚© 63,084"
            r'Start\s+Price\s*\$\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',  # ì˜ì–´ "Start Price $ 123.45"
            r'ì‹œì‘ê°€\s*\$\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',  # í•œêµ­ì–´ "ì‹œì‘ê°€ $ 123.45"
        ]
        
        found_price = None
        for pattern in patterns:
            match = re.search(pattern, all_text, re.IGNORECASE)
            if match:
                price_number = match.group(1)
                currency_symbol = "â‚©" if "â‚©" in pattern else "$"
                
                found_price = {
                    'price': f"{currency_symbol}{price_number}",
                    'context': match.group(0),  # ì „ì²´ ë§¤ì¹˜ëœ í…ìŠ¤íŠ¸
                    'source': 'start_price_simple'
                }
                print(f"âœ… ê°€ê²© ë°œê²¬: {found_price['price']}")
                break
        
        # íŒŒì¼ ì €ì¥ (í•­ìƒ ì €ì¥)
        try:
            import os
            if not os.path.exists('downloads'):
                os.makedirs('downloads')
            
            cid_match = re.search(r'cid=([^&]+)', url)
            cid_value = cid_match.group(1) if cid_match else 'unknown'
            filename = f"page_text_cid_{cid_value}.txt"
            filepath = os.path.join('downloads', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n")
                f.write(f"CID: {cid_value}\n")
                f.write(f"ìŠ¤í¬ë˜í•‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"íŒŒì¼ í¬ê¸°: {len(all_text.encode('utf-8'))} bytes\n")
                f.write("="*50 + "\n\n")
                f.write(all_text)
                
            print(f"íŒŒì¼ ì €ì¥ë¨: {filepath}")
        except:
            pass
        
        # ê²°ê³¼ ë°˜í™˜ (ì°¾ìœ¼ë©´ ë°˜í™˜, ëª» ì°¾ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        if found_price:
            return [found_price]
        else:
            print("âŒ ì‹œì‘ê°€/Start Price ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return []

    except Exception as e:
        print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return []


def process_all_cids_sequential(base_url, cid_list):
    """
    ëª¨ë“  CIDë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ê° ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë°˜í™˜
    """
    original_cid = extract_cid_from_url(base_url)
    total_cids = len(cid_list)
    
    # ì‹œì‘ ì‹ í˜¸
    yield {
        'type': 'start',
        'total_cids': total_cids
    }
    
    # ê° CIDë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for i, new_cid in enumerate(cid_list, 1):
        try:
            # URL ìƒì„±
            if original_cid:
                new_url = base_url.replace(f"cid={original_cid}", f"cid={new_cid}")
            else:
                separator = "&" if "?" in base_url else "?"
                new_url = f"{base_url}{separator}cid={new_cid}"
            
            # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            prices = scrape_prices_simple(new_url, 'USD')
            
            # ê²°ê³¼ ë°˜í™˜
            yield {
                'type': 'progress',
                'step': i,
                'total_steps': total_cids,
                'cid': new_cid,
                'url': new_url,
                'prices': prices,
                'success': len(prices) > 0
            }
            
        except Exception as e:
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì§„í–‰ ê³„ì†
            yield {
                'type': 'progress',
                'step': i,
                'total_steps': total_cids,
                'cid': new_cid,
                'url': new_url if 'new_url' in locals() else base_url,
                'prices': [],
                'success': False,
                'error': str(e)
            }
    
    # ì™„ë£Œ ì‹ í˜¸
    yield {
        'type': 'complete'
    }


def extract_cid_from_url(url):
    """URLì—ì„œ CID íŒŒë¼ë¯¸í„° ê°’ì„ ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        return params.get('cid', [None])[0]
    except:
        return None


def reorder_url_parameters(url):
    """URLì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¼ê´€ëœ ìˆœì„œë¡œ ì¬ì •ë ¬"""
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query, keep_blank_values=True)
        
        # íŒŒë¼ë¯¸í„°ë“¤ì„ ì•ŒíŒŒë²³ ìˆœì„œë¡œ ì •ë ¬
        sorted_params = []
        for key in sorted(params.keys()):
            for value in params[key]:
                sorted_params.append(f"{key}={value}")
        
        # ìƒˆë¡œìš´ URL êµ¬ì„±
        new_query = "&".join(sorted_params)
        new_url = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path,
            parsed.params,
            new_query,
            parsed.fragment
        ))
        
        return new_url
    except:
        return url  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ URL ë°˜í™˜
