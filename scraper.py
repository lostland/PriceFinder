import re
from bs4 import BeautifulSoup
import logging
import requests
import time
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

def scrape_prices_simple(url, original_currency_code=None):
    """
    ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ ê°€ê²© ìŠ¤í¬ë˜í•‘ - ì´ë¯¸ì§€ ì²˜ë¦¬ ì—†ìŒ
    Returns a list of dictionaries containing price and context information
    original_currency_code: ì›ë³¸ URLì˜ í†µí™” ì½”ë“œ (ì˜ˆ: USD, KRW, THB)
    """
    try:
        # ìµœì í™”ëœ Selenium ì‚¬ìš© - AgodaëŠ” JavaScript ì‹¤í–‰ í•„ìš”
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ì°¨ë‹¨
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-web-security')  # ì›¹ ë³´ì•ˆ í•´ì œë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')  # ë Œë”ë§ ìµœì í™”
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-features=TranslateUI')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-features=NetworkService')  # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
        chrome_options.add_argument('--disable-ipc-flooding-protection')  # IPC ìµœì í™”
        chrome_options.add_argument('--window-size=1920,1080')  # ë°ìŠ¤í¬íƒ‘ í¬ê¸°ë¡œ ë³€ê²½  
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--log-level=3')
        chrome_options.add_argument('--silent')
        chrome_options.add_argument('--no-zygote')  # í”„ë¡œì„¸ìŠ¤ ìµœì í™”
        chrome_options.add_argument('--single-process')  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # ë´‡ ê°ì§€ ìš°íšŒ
        
        # ë´‡ íƒì§€ ìš°íšŒ (ê°„ë‹¨í•˜ê²Œ)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ìŠ¤í¬ë˜í•‘ ì‚¬ìš© URL: {url}")
        
        start_time = time.time()
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(6)  # 6ì´ˆë¡œ ì ë‹¹íˆ ë‹¨ì¶•
        
        # ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸ (ë¹ ë¥´ê²Œ)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # ìŠ¤í¬ë¡¤ì„ ë§¨ ìœ„ë¡œ ê³ ì •
        driver.execute_script("window.scrollTo(0, 0);")
        
        try:
            # ğŸ’¡ JavaScript ì‹¤í–‰ì„ ìœ„í•œ ë‹¨ê³„ë³„ ë¡œë”©
            print("ğŸŒ í˜ì´ì§€ ë¡œë”© ì‹œì‘...")
            driver.get(url)
            
            # 1ë‹¨ê³„: 1ì´ˆ ëŒ€ê¸° (JavaScript ì‹¤í–‰ ì‹œê°„ í™•ë³´)
            print("â³ JavaScript ì‹¤í–‰ì„ ìœ„í•´ 1ì´ˆ ëŒ€ê¸°...")
            time.sleep(1)
            
            # 1ì´ˆ í›„ ì²« ë²ˆì§¸ ì†ŒìŠ¤ í™•ì¸
            initial_source = driver.page_source
            initial_text = BeautifulSoup(initial_source, 'html.parser').get_text()
            
            print(f"1ì´ˆ í›„ í…ìŠ¤íŠ¸ í¬ê¸°: {len(initial_text)} ê¸€ì")
            print(f"1ì´ˆ í›„ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {initial_text[:200]}...")
            
            # "ì‹œì‘ê°€" ë˜ëŠ” "Start Price" í‚¤ì›Œë“œ ì²´í¬
            price_keyword_found = ('ì‹œì‘ê°€' in initial_text or 'Start Price' in initial_text)
            
            if price_keyword_found:
                print("ğŸ¯ 1ì´ˆ í›„ ê°€ê²© í‚¤ì›Œë“œ ë°œê²¬!")
                page_source = initial_source
            else:
                print("â° ê°€ê²© í‚¤ì›Œë“œ ì—†ìŒ - ì¶”ê°€ ëŒ€ê¸°...")
                
                # 2ë‹¨ê³„: ìµœëŒ€ 5ì´ˆ ë” ëŒ€ê¸°í•˜ë©´ì„œ ë§¤ 1ì´ˆë§ˆë‹¤ ì²´í¬
                max_additional_wait = 5
                start_additional = time.time()
                
                while time.time() - start_additional < max_additional_wait:
                    time.sleep(1)
                    current_source = driver.page_source
                    current_text = BeautifulSoup(current_source, 'html.parser').get_text()
                    
                    elapsed = int(time.time() - start_additional)
                    print(f"â° +{elapsed}ì´ˆ ê²½ê³¼ - í…ìŠ¤íŠ¸ í¬ê¸°: {len(current_text)} ê¸€ì")
                    
                    # í‚¤ì›Œë“œ ì²´í¬
                    if 'ì‹œì‘ê°€' in current_text or 'Start Price' in current_text:
                        print("ğŸ¯ ê°€ê²© í‚¤ì›Œë“œ ë°œê²¬ - ë°”ë¡œ ì¶”ì¶œ ì§„í–‰")
                        page_source = current_source
                        break
                        
                    # 10KB ë„˜ìœ¼ë©´ ì¤‘ë‹¨
                    if len(current_text.encode('utf-8')) >= 10 * 1024:
                        print("ğŸ“ 10KB ë„ë‹¬ - ë¡œë”© ì¤‘ë‹¨")
                        page_source = current_source
                        break
                        
                    page_source = current_source
                
                if time.time() - start_additional >= max_additional_wait:
                    print("â° ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ ë„ë‹¬")
            
            # ìµœì¢… ì†ŒìŠ¤ í¬ê¸° ì œí•œ (10KB)
            final_text = BeautifulSoup(page_source, 'html.parser').get_text()
            if len(final_text.encode('utf-8')) > 10 * 1024:
                # HTML ì†ŒìŠ¤ ìì²´ë¥¼ 10KBë¡œ ì œí•œ
                page_source = page_source.encode('utf-8')[:10*1024].decode('utf-8', errors='ignore')
                print(f"ğŸ“ 10KBë¡œ ì†ŒìŠ¤ ì˜ë¼ë‚´ê¸° ì™„ë£Œ")
            
        except:
            # íƒ€ì„ì•„ì›ƒë˜ì–´ë„ í˜„ì¬ê¹Œì§€ ë¡œë”©ëœ ì†ŒìŠ¤ë¼ë„ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
        finally:
            driver.quit()
        
        load_time = time.time() - start_time
        print(f"âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        parse_start = time.time()
        soup = BeautifulSoup(page_source, 'html.parser')
        parse_time = time.time() - parse_start
        
        # ğŸ” ë””ë²„ê¹…: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê³¼ì • ê¸°ë¡
        debug_info = []
        debug_info.append(f"=== í…ìŠ¤íŠ¸ ì¶”ì¶œ ë””ë²„ê·¸ ì •ë³´ ===")
        debug_info.append(f"íŒŒì‹± ì‹œê°„: {parse_time:.3f}ì´ˆ")
        debug_info.append(f"í˜ì´ì§€ ì†ŒìŠ¤ í¬ê¸°: {len(page_source)} characters")
        debug_info.append(f"ì¶”ì¶œ ì‹œì‘ ì‹œê°„: {time.strftime('%H:%M:%S')}")
        debug_info.append(f"")
        
        # ğŸ“Š ë°©ë²•1: ì „ì²´ í…ìŠ¤íŠ¸ (ê¸°ë³¸ ë°©ì‹)
        method1_start = time.time()
        all_text_full = soup.get_text()
        method1_time = time.time() - method1_start
        debug_info.append(f"ë°©ë²•1 - soup.get_text() ì „ì²´:")
        debug_info.append(f"  ì‹œê°„: {method1_time:.3f}ì´ˆ")
        debug_info.append(f"  í¬ê¸°: {len(all_text_full)} ê¸€ì")
        debug_info.append(f"  ì•ìª½ ë¯¸ë¦¬ë³´ê¸°: {all_text_full[:200]}...")
        debug_info.append(f"")
        
        # ğŸ“Š ë°©ë²•2: body íƒœê·¸ë§Œ
        method2_start = time.time()
        body_tag = soup.find('body')
        if body_tag:
            body_text = body_tag.get_text()
            method2_time = time.time() - method2_start
            debug_info.append(f"ë°©ë²•2 - body.get_text():")
            debug_info.append(f"  ì‹œê°„: {method2_time:.3f}ì´ˆ")
            debug_info.append(f"  í¬ê¸°: {len(body_text)} ê¸€ì")
            debug_info.append(f"  ì•ìª½ ë¯¸ë¦¬ë³´ê¸°: {body_text[:200]}...")
        else:
            body_text = ""
            debug_info.append(f"ë°©ë²•2 - body íƒœê·¸ ì—†ìŒ")
        debug_info.append(f"")
        
        # ğŸ“Š ë°©ë²•3: ìƒìœ„ 10ê°œ divë§Œ
        method3_start = time.time()
        top_divs = soup.find_all('div')[:10]
        div_text = ""
        for i, div in enumerate(top_divs):
            div_content = div.get_text(strip=True)
            if div_content:
                div_text += f"[DIV{i+1}] {div_content}\n"
        method3_time = time.time() - method3_start
        debug_info.append(f"ë°©ë²•3 - ìƒìœ„ 10ê°œ div:")
        debug_info.append(f"  ì‹œê°„: {method3_time:.3f}ì´ˆ")
        debug_info.append(f"  í¬ê¸°: {len(div_text)} ê¸€ì")
        debug_info.append(f"  ì•ìª½ ë¯¸ë¦¬ë³´ê¸°: {div_text[:200]}...")
        debug_info.append(f"")
        
        # ğŸ“Š ë°©ë²•4: JavaScriptë¡œ í™”ë©´ì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ ì‹œë„
        method4_start = time.time()
        try:
            visible_text = driver.execute_script("""
                // í™”ë©´ì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                var walker = document.createTreeWalker(
                    document.body,
                    NodeFilter.SHOW_TEXT,
                    {
                        acceptNode: function(node) {
                            var parent = node.parentElement;
                            var style = window.getComputedStyle(parent);
                            
                            // ìˆ¨ê²¨ì§„ ìš”ì†ŒëŠ” ì œì™¸
                            if (style.display === 'none' || style.visibility === 'hidden' || style.opacity === '0') {
                                return NodeFilter.FILTER_REJECT;
                            }
                            
                            // í™”ë©´ ì˜ì—­ ë°–ì€ ì œì™¸ (ëŒ€ëµì ìœ¼ë¡œ)
                            var rect = parent.getBoundingClientRect();
                            if (rect.bottom < 0 || rect.top > window.innerHeight * 2) {
                                return NodeFilter.FILTER_REJECT;
                            }
                            
                            return NodeFilter.FILTER_ACCEPT;
                        }
                    }
                );
                
                var textNodes = [];
                var node;
                while (node = walker.nextNode()) {
                    if (node.nodeValue.trim()) {
                        textNodes.push(node.nodeValue.trim());
                    }
                }
                return textNodes.join(' ');
            """)
            method4_time = time.time() - method4_start
            debug_info.append(f"ë°©ë²•4 - JavaScript í™”ë©´ í…ìŠ¤íŠ¸:")
            debug_info.append(f"  ì‹œê°„: {method4_time:.3f}ì´ˆ")
            debug_info.append(f"  í¬ê¸°: {len(visible_text)} ê¸€ì")
            debug_info.append(f"  ì•ìª½ ë¯¸ë¦¬ë³´ê¸°: {visible_text[:200]}...")
        except Exception as js_error:
            visible_text = ""
            debug_info.append(f"ë°©ë²•4 - JavaScript ì‹¤íŒ¨: {js_error}")
        debug_info.append(f"")
        
        # ğŸ¯ ìµœì¢… ì„ íƒ: ê°€ì¥ ì ì ˆí•œ í…ìŠ¤íŠ¸ ì„ íƒ
        debug_info.append(f"=== ìµœì¢… ì„ íƒ ===")
        
        if visible_text and len(visible_text) > 100:
            all_text = visible_text
            debug_info.append(f"ì„ íƒ: ë°©ë²•4 - JavaScript í™”ë©´ í…ìŠ¤íŠ¸")
        elif body_text and len(body_text) > 100:
            all_text = body_text[:10240]  # ì²« 10KBë§Œ
            debug_info.append(f"ì„ íƒ: ë°©ë²•2 - body í…ìŠ¤íŠ¸ (10KB ì œí•œ)")
        else:
            all_text = all_text_full[:10240]  # ì²« 10KBë§Œ
            debug_info.append(f"ì„ íƒ: ë°©ë²•1 - ì „ì²´ í…ìŠ¤íŠ¸ (10KB ì œí•œ)")
        
        debug_info.append(f"ìµœì¢… í¬ê¸°: {len(all_text)} ê¸€ì, {len(all_text.encode('utf-8'))} bytes")
        debug_info.append(f"=" * 50)
        
        print(f"í˜ì´ì§€ í¬ê¸°: {len(all_text)} ê¸€ì, {len(all_text.encode('utf-8'))} bytes")
        
        # ğŸ¯ ë‹¨ìˆœí™”: "ì‹œì‘ê°€" ë˜ëŠ” "Start Price"ë§Œ ì°¾ê¸°
        patterns = [
            r'ì‹œì‘ê°€\s*â‚©\s*(\d{1,3}(?:,\d{3})+)',  # í•œêµ­ì–´ "ì‹œì‘ê°€ â‚© 63,084"
            r'Start\s+Price\s*\$\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',  # ì˜ì–´ "Start Price $ 123.45"
            r'ì‹œì‘ê°€\s*\$\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',  # í•œêµ­ì–´ "ì‹œì‘ê°€ $ 123.45"
        ]
        
        found_price = None
        for pattern in patterns:
            match = re.search(pattern, all_text, re.IGNORECASE)
            if match:
                price_number = match.group(1)
                currency_symbol = "â‚©" if "â‚©" in pattern else "$"
                
                found_price = {
                    'price': f"{currency_symbol}{price_number}",
                    'context': match.group(0),  # ì „ì²´ ë§¤ì¹˜ëœ í…ìŠ¤íŠ¸
                    'source': 'start_price_simple'
                }
                print(f"âœ… ê°€ê²© ë°œê²¬: {found_price['price']}")
                break
        
        # íŒŒì¼ ì €ì¥ (ë””ë²„ê·¸ ì •ë³´ í¬í•¨)
        try:
            import os
            if not os.path.exists('downloads'):
                os.makedirs('downloads')
            
            cid_match = re.search(r'cid=([^&]+)', url)
            cid_value = cid_match.group(1) if cid_match else 'unknown'
            filename = f"page_text_cid_{cid_value}.txt"
            filepath = os.path.join('downloads', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n")
                f.write(f"CID: {cid_value}\n")
                f.write(f"ìŠ¤í¬ë˜í•‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì´ ì†Œìš” ì‹œê°„: {load_time:.2f}ì´ˆ\n")
                f.write(f"íŒŒì¼ í¬ê¸°: {len(all_text.encode('utf-8'))} bytes\n")
                f.write("\n" + "\n".join(debug_info) + "\n\n")
                f.write("=== ì‹¤ì œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ===\n")
                f.write(all_text)
                
            print(f"íŒŒì¼ ì €ì¥ë¨: {filepath} (ë””ë²„ê·¸ ì •ë³´ í¬í•¨)")
        except Exception as save_error:
            print(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {save_error}")
        
        # ê²°ê³¼ ë°˜í™˜ (ì°¾ìœ¼ë©´ ë°˜í™˜, ëª» ì°¾ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        if found_price:
            return [found_price]
        else:
            print("âŒ ì‹œì‘ê°€/Start Price ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return []

    except Exception as e:
        print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return []


def process_all_cids_sequential(base_url, cid_list):
    """
    ëª¨ë“  CIDë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ê° ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë°˜í™˜
    """
    original_cid = extract_cid_from_url(base_url)
    total_cids = len(cid_list)
    
    # ì‹œì‘ ì‹ í˜¸
    yield {
        'type': 'start',
        'total_cids': total_cids
    }
    
    # ê° CIDë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for i, new_cid in enumerate(cid_list, 1):
        try:
            # URL ìƒì„±
            if original_cid:
                new_url = base_url.replace(f"cid={original_cid}", f"cid={new_cid}")
            else:
                separator = "&" if "?" in base_url else "?"
                new_url = f"{base_url}{separator}cid={new_cid}"
            
            # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            prices = scrape_prices_simple(new_url, 'USD')
            
            # ê²°ê³¼ ë°˜í™˜
            yield {
                'type': 'progress',
                'step': i,
                'total_steps': total_cids,
                'cid': new_cid,
                'url': new_url,
                'prices': prices,
                'success': len(prices) > 0
            }
            
        except Exception as e:
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì§„í–‰ ê³„ì†
            yield {
                'type': 'progress',
                'step': i,
                'total_steps': total_cids,
                'cid': new_cid,
                'url': new_url if 'new_url' in locals() else base_url,
                'prices': [],
                'success': False,
                'error': str(e)
            }
    
    # ì™„ë£Œ ì‹ í˜¸
    yield {
        'type': 'complete'
    }


def extract_cid_from_url(url):
    """URLì—ì„œ CID íŒŒë¼ë¯¸í„° ê°’ì„ ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        return params.get('cid', [None])[0]
    except:
        return None


def reorder_url_parameters(url):
    """URLì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¼ê´€ëœ ìˆœì„œë¡œ ì¬ì •ë ¬"""
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query, keep_blank_values=True)
        
        # íŒŒë¼ë¯¸í„°ë“¤ì„ ì•ŒíŒŒë²³ ìˆœì„œë¡œ ì •ë ¬
        sorted_params = []
        for key in sorted(params.keys()):
            for value in params[key]:
                sorted_params.append(f"{key}={value}")
        
        # ìƒˆë¡œìš´ URL êµ¬ì„±
        new_query = "&".join(sorted_params)
        new_url = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path,
            parsed.params,
            new_query,
            parsed.fragment
        ))
        
        return new_url
    except:
        return url  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ URL ë°˜í™˜
