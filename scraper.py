import re
from bs4 import BeautifulSoup
import logging
import requests
import time
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

def scrape_prices_simple(url, original_currency_code=None):
    """
    ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ ê°€ê²© ìŠ¤í¬ë˜í•‘ - ì´ë¯¸ì§€ ì²˜ë¦¬ ì—†ìŒ
    Returns a list of dictionaries containing price and context information
    original_currency_code: ì›ë³¸ URLì˜ í†µí™” ì½”ë“œ (ì˜ˆ: USD, KRW, THB)
    """
    try:
        # ìµœì í™”ëœ Selenium ì‚¬ìš© - AgodaëŠ” JavaScript ì‹¤í–‰ í•„ìš”
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ì°¨ë‹¨
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-web-security')  # ì›¹ ë³´ì•ˆ í•´ì œë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')  # ë Œë”ë§ ìµœì í™”
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-features=TranslateUI')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-features=NetworkService')  # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
        chrome_options.add_argument('--disable-ipc-flooding-protection')  # IPC ìµœì í™”
        chrome_options.add_argument('--window-size=320,240')  # ë”ë”ìš± ì‘ì€ ì°½  
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--log-level=3')
        chrome_options.add_argument('--silent')
        chrome_options.add_argument('--no-zygote')  # í”„ë¡œì„¸ìŠ¤ ìµœì í™”
        chrome_options.add_argument('--single-process')  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # ë´‡ ê°ì§€ ìš°íšŒ
        
        # ë´‡ íƒì§€ ìš°íšŒ (ê°„ë‹¨í•˜ê²Œ)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ìŠ¤í¬ë˜í•‘ ì‚¬ìš© URL: {url}")
        
        start_time = time.time()
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(6)  # 6ì´ˆë¡œ ì ë‹¹íˆ ë‹¨ì¶•
        
        # ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸ (ë¹ ë¥´ê²Œ)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # í˜ì´ì§€ ë¡œë”© ìµœì í™”
            driver.get(url)
            
            # DOMì´ ì¤€ë¹„ë˜ë©´ ì¦‰ì‹œ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            
            # í˜ì´ì§€ ê¸°ë³¸ ë¡œë”© ì™„ë£Œ ëŒ€ê¸° (ìµœì†Œí•œ)
            WebDriverWait(driver, 3).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            page_source = driver.page_source
            
        except:
            # íƒ€ì„ì•„ì›ƒë˜ì–´ë„ í˜„ì¬ê¹Œì§€ ë¡œë”©ëœ ì†ŒìŠ¤ë¼ë„ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
        finally:
            driver.quit()
        
        load_time = time.time() - start_time
        print(f"âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(page_source, 'html.parser')
        all_text = soup.get_text()
        
        print(f"í˜ì´ì§€ í¬ê¸°: {len(all_text)} ê¸€ì, {len(all_text.encode('utf-8'))} bytes")
        
        # ğŸ¯ í•µì‹¬ ìˆ˜ì •: ì‹œì‘ê°€ ê²€ìƒ‰ì„ ëª¨ë“  ë¡œì§ë³´ë‹¤ ìš°ì„ ìœ¼ë¡œ
        pattern = r'ì‹œì‘ê°€\s*â‚©\s*(\d{1,3}(?:,\d{3})+)'
        match = re.search(pattern, all_text)
        
        if match:
            price_number = match.group(1)
            starting_price = {
                'price': f"â‚©{price_number}",
                'context': f"ì‹œì‘ê°€ â‚©{price_number}",
                'source': 'starting_price_direct'
            }
            print(f"âœ… ì‹œì‘ê°€ ë°œê²¬: {starting_price['price']}")
            
            # íŒŒì¼ ì €ì¥ (ë””ë²„ê·¸ìš©)
            try:
                import os
                if not os.path.exists('downloads'):
                    os.makedirs('downloads')
                
                cid_match = re.search(r'cid=([^&]+)', url)
                cid_value = cid_match.group(1) if cid_match else 'unknown'
                filename = f"page_text_cid_{cid_value}.txt"
                filepath = os.path.join('downloads', filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"URL: {url}\n")
                    f.write(f"CID: {cid_value}\n")
                    f.write(f"ìŠ¤í¬ë˜í•‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"íŒŒì¼ í¬ê¸°: {len(all_text.encode('utf-8'))} bytes\n")
                    f.write("="*50 + "\n\n")
                    f.write(all_text)  # ğŸ’¡ ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥ (5000ì ì œí•œ ì œê±°)
                    
                print(f"íŒŒì¼ ì €ì¥ë¨: {filepath}")
            except:
                pass
            
            # ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
            return [starting_price]
        else:
            print("âŒ ì‹œì‘ê°€ íŒ¨í„´ ì‹¤íŒ¨ - ì¼ë°˜ ê°€ê²© ê²€ìƒ‰ ì§„í–‰")
        
        prices_found = []
        seen_prices = set()
        
        # 1ë‹¨ê³„: íŠ¹ì • ê°€ê²© ìš”ì†Œë“¤ë¶€í„° ìš°ì„  ì°¾ê¸° (ì‹¤ì œ ì˜ˆì•½ ê°€ê²©)
        price_selectors = [
            # ì¼ë°˜ì ì¸ í˜¸í…” ì˜ˆì•½ ì‚¬ì´íŠ¸ ê°€ê²© í´ë˜ìŠ¤ë“¤
            '[class*="price"]',
            '[class*="cost"]', 
            '[class*="rate"]',
            '[class*="amount"]',
            '[class*="total"]',
            '[class*="nightly"]',
            '[data-testid*="price"]',
            '[data-price]',
            # ë” êµ¬ì²´ì ì¸ ì…€ë ‰í„°ë“¤
            '.room-price',
            '.hotel-price',
            '.booking-price',
            '.final-price'
        ]
        
        for selector in price_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(strip=True)
                    
                    # ê°€ê²© íŒ¨í„´ ì°¾ê¸°
                    price_patterns = [
                        r'(\$[1-9]\d{2,4}(?:\.\d{2})?)',  # $100-99999.99
                        r'([1-9]\d{2,4}(?:\.\d{2})?\s*USD)',  # 123.45 USD
                        r'(\$[1-9]\d{1,2})',  # $10-999
                    ]
                    
                    for pattern in price_patterns:
                        matches = re.findall(pattern, text, re.IGNORECASE)
                        for price_text in matches:
                            if price_text not in seen_prices:
                                # í‰ê· ê°€ê²© ì œì™¸
                                parent_text = element.parent.get_text(strip=True).lower() if element.parent else text.lower()
                                
                                is_average_price = (
                                    'average' in parent_text or
                                    'avg' in parent_text or
                                    'stands at' in parent_text or
                                    'typical' in parent_text
                                )
                                
                                if not is_average_price:
                                    seen_prices.add(price_text)
                                    prices_found.append({
                                        'price': price_text,
                                        'context': f"Found in {selector}: {text[:100]}",
                                        'source': 'targeted_element'
                                    })
                                    
                                    if len(prices_found) >= 3:
                                        break
                        
                        if len(prices_found) >= 3:
                            break
                    
                    if len(prices_found) >= 3:
                        break
            except Exception:
                continue
            
            if len(prices_found) >= 3:
                break
        
        # 2ë‹¨ê³„: íŠ¹ì • ìš”ì†Œì—ì„œ ëª» ì°¾ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        if len(prices_found) < 2:
            # scriptì™€ style íƒœê·¸ ì œê±°
            for element in soup(["script", "style"]):
                element.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = soup.get_text()
            
            # ë” ì ê·¹ì ì¸ ê°€ê²© íŒ¨í„´ ê²€ìƒ‰
            price_patterns = [
                # ì‹¤ì œ ì˜ˆì•½ ê°€ê²©ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ íŒ¨í„´ë“¤
                r'(\$[1-9]\d{2,4}(?:\.\d{2})?)\s*(?:per night|night|/night)',  # $123 per night
                r'(\$[1-9]\d{2,4}(?:\.\d{2})?)\s*(?:total|Total)',  # $123 total
                r'(?:from|From)\s*(\$[1-9]\d{2,4}(?:\.\d{2})?)',  # from $123
                r'(\$[1-9]\d{2,4}(?:\.\d{2})?)',  # ì¼ë°˜ $123
                r'([1-9]\d{2,4}(?:\.\d{2})?\s*USD)',  # 123 USD
            ]
            
            for pattern in price_patterns:
                matches = re.finditer(pattern, text_content, re.IGNORECASE)
                
                for match in matches:
                    price_text = match.group(1).strip()
                    
                    if price_text in seen_prices:
                        continue
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    start_pos = max(0, match.start() - 80)
                    end_pos = min(len(text_content), match.end() + 80)
                    context = text_content[start_pos:end_pos].strip()
                    context_lower = context.lower()
                    
                    # í‰ê· ê°€ê²© ë° ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ê°€ê²© ì œì™¸
                    skip_keywords = [
                        'with an average room price of',
                        'which stands at',
                        'average room price',
                        'typical price',
                        'generally costs',
                        'usually costs'
                    ]
                    
                    should_skip = any(keyword in context_lower for keyword in skip_keywords)
                    
                    if should_skip:
                        continue
                    
                    context = re.sub(r'\s+', ' ', context)[:150]
                    
                    seen_prices.add(price_text)
                    prices_found.append({
                        'price': price_text,
                        'context': context,
                        'source': 'text_search'
                    })
                    
                    # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
                    if len(prices_found) >= 5:
                        break
                
                if len(prices_found) >= 5:
                    break
        
        # ğŸ§¹ ì¤‘ë³µ ë¡œì§ ëª¨ë‘ ì œê±° ì™„ë£Œ
        # ìœ„ì—ì„œ ì´ë¯¸ ì‹œì‘ê°€ ê²€ìƒ‰ì„ ì™„ë£Œí–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¼ë°˜ ê°€ê²©ë§Œ ê²€ìƒ‰
        
        # ë” ê´‘ë²”ìœ„í•œ ê°€ê²© íŒ¨í„´ ê²€ìƒ‰
        all_price_patterns = [
            # ë‹¬ëŸ¬ íŒ¨í„´
            r'(\$[1-9]\d{2,4}(?:\.\d{2})?)',  # $100-99999.99
            r'(\$[1-9]\d{1,2})',  # $10-999
            # USD íŒ¨í„´
            r'([1-9]\d{2,4}(?:\.\d{2})?\s*USD)',  # 100-9999.99 USD
            r'USD\s*([1-9]\d{2,4}(?:\.\d{2})?)',  # USD 100-9999.99
            r'USD\s*([1-9]\d{1,2})',  # USD 10-999
            # ìˆœìˆ˜ ìˆ«ì íŒ¨í„´ (ê°€ê²©ì¼ ê°€ëŠ¥ì„±)
            r'\b([2-9]\d{2})\b',  # 200-999 (3ìë¦¬)
            r'\b([1-9]\d{3})\b',  # 1000-9999 (4ìë¦¬)
        ]
        
        all_prices = []
        
        for pattern in all_price_patterns:
            matches = re.finditer(pattern, all_text, re.IGNORECASE)
            for match in matches:
                price_text = match.group(1).strip()
                
                if price_text not in seen_prices:
                    context_start = max(0, match.start() - 60)
                    context_end = min(len(all_text), match.end() + 60)
                    context = all_text[context_start:context_end].strip()
                    context_lower = context.lower()
                    context = re.sub(r'\s+', ' ', context)[:150]
                    
                    # í‰ê· ê°€ê²© ê°•í™” í•„í„°ë§ 
                    is_average_price = (
                        'with an average room price of' in context_lower or
                        'which stands at' in context_lower or
                        'í‰ê·  ê°ì‹¤ ê°€ê²©ì€' in context_lower or
                        'average room price' in context_lower or
                        'í‰ê·  ê°€ê²©' in context_lower or
                        'í‰ê·  ê°ì‹¤' in context_lower or
                        'ë°©ì½•ì˜ í‰ê· ' in context_lower
                    )
                    
                    # ëª…ë°±í•œ IDë‚˜ ë‚ ì§œë§Œ ì œì™¸
                    is_not_price = (
                        any(year in context for year in ['2024', '2025', '2026']) or
                        (price_text.isdigit() and len(price_text) > 4)  # ê¸´ IDë§Œ ì œì™¸
                    )
                    
                    if not is_average_price and not is_not_price:
                        seen_prices.add(price_text)
                        all_prices.append({
                            'price': f"${price_text}" if not price_text.startswith('$') else price_text,
                            'context': context,
                            'source': 'all_price_search'
                        })
                        
                        # ë” ë§ì´ ìˆ˜ì§‘ (ë‘ ë²ˆì§¸ ê°€ê²©ì„ ì°¾ê¸° ìœ„í•´)
                        if len(all_prices) >= 20:
                            break
            
            if len(all_prices) >= 20:
                break
        
        # ê°€ê²© ë¶„ì„ ì „ì— ë¨¼ì € ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ë‹¤ìš´ë¡œë“œìš©)
        try:
            import os
            if not os.path.exists('downloads'):
                os.makedirs('downloads')
            
            # CID ì •ë³´ ì¶”ì¶œ
            cid_match = re.search(r'cid=([^&]+)', url)
            cid_value = cid_match.group(1) if cid_match else 'unknown'
            
            # íŒŒì¼ëª… ìƒì„±
            filename = f"page_text_cid_{cid_value}.txt"
            filepath = os.path.join('downloads', filename)
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n")
                f.write(f"CID: {cid_value}\n")
                f.write(f"ìŠ¤í¬ë˜í•‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*50 + "\n\n")
                f.write(all_text)
            
            print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ë¨: {filepath}")
            
        except Exception as save_error:
            print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {save_error}")
        
        # ë‘ ë²ˆì§¸ ê°€ê²©ë§Œ ë°˜í™˜ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
        if len(all_prices) >= 2:
            prices_found = [all_prices[1]]  # ë‘ ë²ˆì§¸ ê°€ê²©ë§Œ
        elif len(all_prices) >= 1:
            prices_found = [all_prices[0]]  # ì²« ë²ˆì§¸ë¼ë„ ë°˜í™˜
        else:
            prices_found = all_prices  # ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        
        return prices_found
        
    except Exception as e:
        return []

def process_all_cids_sequential(base_url, cid_list):
    """
    ëª¨ë“  CIDë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ê° ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë°˜í™˜
    Generator that yields results immediately as they become available
    """
    original_cid = extract_cid_from_url(base_url)
    total_cids = len(cid_list)
    
    # ì‹œì‘ ì‹ í˜¸
    yield {
        'type': 'start',
        'total_cids': total_cids
    }
    
    # ê° CIDë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for i, new_cid in enumerate(cid_list, 1):
        try:
            # URL ìƒì„±
            if original_cid:
                new_url = base_url.replace(f"cid={original_cid}", f"cid={new_cid}")
            else:
                separator = "&" if "?" in base_url else "?"
                new_url = f"{base_url}{separator}cid={new_cid}"
            
            # CID ë¼ë²¨ ìƒì„±
            if i == 1:
                cid_label = f"ì›ë³¸({new_cid})"
            else:
                cid_label = str(new_cid)
            
            # ì§„í–‰ë¥  ì •ë³´
            yield {
                'type': 'progress',
                'step': i,
                'total': total_cids,
                'cid': cid_label
            }
            
            # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            start_time = time.time()
            prices = scrape_prices_simple(new_url)
            process_time = time.time() - start_time
            
            # ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
            result = {
                'type': 'result',
                'step': i,
                'total': total_cids,
                'cid': cid_label,
                'url': new_url,
                'prices': prices,
                'found_count': len(prices),
                'process_time': round(process_time, 1)
            }
            
            yield result
            
        except Exception as e:
            yield {
                'type': 'error',
                'step': i,
                'total': total_cids,
                'cid': new_cid,
                'error': str(e)
            }
    
    # ì™„ë£Œ ì‹ í˜¸
    yield {
        'type': 'complete',
        'total_results': total_cids
    }

def extract_cid_from_url(url):
    """URLì—ì„œ CID ê°’ ì¶”ì¶œ"""
    match = re.search(r'cid=([^&]+)', url)
    return match.group(1) if match else None

def reorder_url_parameters(url):
    """
    URLì˜ íŒŒë¼ë©”í„°ë¥¼ ì§€ì •ëœ ìˆœì„œë¡œ ì¬ì •ë ¬í•˜ê³  í•„ìš”í•œ íŒŒë¼ë©”í„°ë§Œ ìœ ì§€
    """
    # ì§€ì •ëœ íŒŒë¼ë©”í„° ìˆœì„œ (í•„ìš”í•œ ëª¨ë“  íŒŒë¼ë¯¸í„° í¬í•¨)
    desired_order = [
        'countryId',
        'finalPriceView', 
        'isShowMobileAppPrice',
        'familyMode',
        'adults',
        'children',
        'childs',  # childrenì˜ ë‹¤ë¥¸ í‘œí˜„
        'maxRooms',
        'rooms',
        'checkIn',    # ì²´í¬ì¸ ë‚ ì§œ (camelCase)
        'checkin',    # ì²´í¬ì¸ ë‚ ì§œ (lowercase)
        'checkOut',   # ì²´í¬ì•„ì›ƒ ë‚ ì§œ (camelCase)
        'checkout',   # ì²´í¬ì•„ì›ƒ ë‚ ì§œ (lowercase)
        'isCalendarCallout',
        'childAges',
        'numberOfGuest',
        'missingChildAges',
        'travellerType',
        'showReviewSubmissionEntry',
        'currencyCode',
        'currency',
        'isFreeOccSearch',
        'los',
        'textToSearch',  # ê²€ìƒ‰ í…ìŠ¤íŠ¸ ì¶”ê°€
        'productType',   # ìƒí’ˆ íƒ€ì… ì¶”ê°€
        'searchrequestid',
        'ds',           # ds íŒŒë¼ë¯¸í„° ì¶”ê°€
        'cid'
    ]
    
    try:
        # URL íŒŒì‹±
        parsed_url = urlparse(url)
        query_string = parsed_url.query
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ íŒŒë¼ë©”í„° ì¶”ì¶œ (ë””ì½”ë”© ì—†ì´)
        params_dict = {}
        
        # ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì„ &ë¡œ ë¶„ë¦¬í•˜ì—¬ íŒŒë¼ë©”í„° ì¶”ì¶œ
        if query_string:
            param_pairs = query_string.split('&')
            for pair in param_pairs:
                if '=' in pair:
                    key, value = pair.split('=', 1)
                    params_dict[key] = value
        
        # ì²´í¬ì¸ ê´€ë ¨ íŒŒë¼ë¯¸í„° í™•ì¸ (ê°„ì†Œí™”)
        checkin_found = [f"{k}={v}" for k, v in params_dict.items() if 'checkin' in k.lower()]
        if checkin_found:
            print(f"ì²´í¬ì¸ ê´€ë ¨ íŒŒë¼ë¯¸í„° ë°œê²¬: {', '.join(checkin_found)}")
        
        # currency íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ KRW ì¶”ê°€
        if 'currency' not in params_dict:
            params_dict['currency'] = 'KRW'
            print("currency íŒŒë¼ë¯¸í„°ê°€ ì—†ì–´ì„œ currency=KRWë¡œ ê¸°ë³¸ê°’ ì¶”ê°€")
        
        # ìƒˆë¡œìš´ íŒŒë¼ë©”í„° ë”•ì…”ë„ˆë¦¬ (ì§€ì •ëœ ìˆœì„œëŒ€ë¡œ)
        reordered_params = {}
        
        # ì§€ì •ëœ ìˆœì„œëŒ€ë¡œ íŒŒë¼ë©”í„° ì¶”ê°€ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ)
        for param in desired_order:
            if param in params_dict:
                reordered_params[param] = params_dict[param]
        
        # ìƒˆë¡œìš´ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ ìƒì„±
        query_parts = []
        for key, value in reordered_params.items():
            query_parts.append(f"{key}={value}")
        new_query = "&".join(query_parts)
        
        # ìƒˆë¡œìš´ URL êµ¬ì„±
        new_url = urlunparse((
            parsed_url.scheme,
            parsed_url.netloc,
            parsed_url.path,
            parsed_url.params,
            new_query,
            parsed_url.fragment
        ))
        
        return new_url
        
    except Exception as e:
        print(f"URL íŒŒë¼ë©”í„° ì¬ì •ë ¬ ì˜¤ë¥˜: {e}")
        return url  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ URL ë°˜í™˜

def replace_cid_and_scrape(base_url, cid_list):
    """ê¸°ì¡´ í•¨ìˆ˜ëª… í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    return process_all_cids_sequential(base_url, cid_list)