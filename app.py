import os
import logging
from urllib.parse import urlparse, parse_qs
from flask import Flask, render_template, request, jsonify, Response, send_file
from werkzeug.middleware.proxy_fix import ProxyFix
from scraper import process_all_cids_sequential

logging.basicConfig(level=logging.INFO)

# create the app
app = Flask(__name__)
app.secret_key = os.environ.get("SESSION_SECRET", "default_secret_key_for_development")
app.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)

@app.route('/')
def index():
    """Main page with URL input form"""
    return render_template('index.html')

@app.route('/scrape', methods=['POST'])
def scrape():
    """Handle single CID scraping request"""
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        step = data.get('step', 0)  # í˜„ì¬ ë‹¨ê³„ (0ë¶€í„° ì‹œì‘)
        
        if not url:
            return jsonify({'error': 'URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”'}), 400
        
        # Add protocol if missing
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # [ê²€ìƒ‰ì°½ë¦¬ìŠ¤íŠ¸] CID ê°’ë“¤
        search_cids = [
            ('-1', 'ì‹œí¬ë¦¿ì°½'),
            ('1829968', 'êµ¬ê¸€ì§€ë„A'),
            ('1917614', 'êµ¬ê¸€ì§€ë„B'),
            ('1833981', 'êµ¬ê¸€ì§€ë„C'),
            ('1776688', 'êµ¬ê¸€ ê²€ìƒ‰A'),
            ('1922868', 'êµ¬ê¸€ ê²€ìƒ‰B'),
            ('1908612', 'êµ¬ê¸€ ê²€ìƒ‰C'),
            ('1729890', 'ë„¤ì´ë²„ ê²€ìƒ‰'),
            ('1587497', 'TripAdvisor')
        ]
        
        # [ì¹´ë“œë¦¬ìŠ¤íŠ¸] CID ê°’ë“¤
        card_cids = [
            ('1942636', 'ì¹´ì¹´ì˜¤í˜ì´'),
            ('1895693', 'í˜„ëŒ€ì¹´ë“œ'),
            ('1563295', 'êµ­ë¯¼ì¹´ë“œ'),
            ('1654104', 'ìš°ë¦¬ì¹´ë“œ'),
            ('1748498', 'BCì¹´ë“œ'),
            ('1760133', 'ì‹ í•œì¹´ë“œ'),
            ('1729471', 'í•˜ë‚˜ì¹´ë“œ'),
            ('1917334', 'í† ìŠ¤')
        ]
        
        # ëª¨ë“  CIDë¥¼ í•©ì¹œ ë¦¬ìŠ¤íŠ¸
        all_cids = search_cids + card_cids
        
        # ìœ íš¨í•œ ë‹¨ê³„ì¸ì§€ í™•ì¸
        if step >= len(all_cids):
            return jsonify({'error': 'ëª¨ë“  CID ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤'}), 400
        
        current_cid, current_name = all_cids[step]
        
        # URLì—ì„œ CID êµì²´í•˜ê³  currencyCode ìœ ì§€
        from scraper import extract_cid_from_url, scrape_prices_simple, reorder_url_parameters
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
        import re
        
        # ì›ë³¸ URLì—ì„œ currencyCode ì¶”ì¶œ
        original_currency = None
        currency_match = re.search(r'currencyCode=([^&]+)', url)
        if currency_match:
            original_currency = currency_match.group(1)
        
        original_cid = extract_cid_from_url(url)
        if original_cid:
            new_url = url.replace(f"cid={original_cid}", f"cid={current_cid}")
        else:
            separator = "&" if "?" in url else "?"
            new_url = f"{url}{separator}cid={current_cid}"
        
        # currencyCodeê°€ ë°”ë€Œì—ˆë‹¤ë©´ ì›ë³¸ìœ¼ë¡œ ë³µì›
        if original_currency:
            # í˜„ì¬ URLì˜ currencyCode ì°¾ì•„ì„œ êµì²´
            current_currency_match = re.search(r'currencyCode=([^&]+)', new_url)
            if current_currency_match:
                current_currency = current_currency_match.group(1)
                if current_currency != original_currency:
                    new_url = new_url.replace(f"currencyCode={current_currency}", f"currencyCode={original_currency}")
                    app.logger.info(f"CurrencyCode ë³µì›: {current_currency} â†’ {original_currency}")
            else:
                # currencyCodeê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                separator = "&" if "?" in new_url else "?"
                new_url = f"{new_url}{separator}currencyCode={original_currency}"
                app.logger.info(f"CurrencyCode ì¶”ê°€: {original_currency}")
        
        # URL íŒŒë¼ë©”í„°ë¥¼ ì§€ì •ëœ ìˆœì„œë¡œ ì¬ì •ë ¬
        app.logger.info(f"ì¬ì •ë ¬ ì „ URL: {new_url}")
        new_url = reorder_url_parameters(new_url)
        app.logger.info(f"ì¬ì •ë ¬ í›„ URL: {new_url}")
        
        # ì§„í–‰ ë‹¨ê³„ ì •ë³´
        is_search_phase = step < len(search_cids)
        phase_name = "ê²€ìƒ‰ì°½ë¦¬ìŠ¤íŠ¸" if is_search_phase else "ì¹´ë“œë¦¬ìŠ¤íŠ¸"
        
        app.logger.info(f"Processing step {step+1}/{len(all_cids)}: CID {current_name}({current_cid})")
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ (ì›ë³¸ currencyCode ì „ë‹¬)
        import time
        start_time = time.time()
        prices = scrape_prices_simple(new_url, original_currency_code=original_currency)
        process_time = time.time() - start_time
        
        # ì²«ë²ˆì§¸ CIDì—ì„œ ê°€ê²©ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° - ì˜ëª»ëœ ë§í¬ë¡œ íŒë‹¨
        if step == 0 and len(prices) == 0:
            app.logger.warning(f"First CID parsing failed - no prices found")
            app.logger.warning(f"Original URL: {url}")
            app.logger.warning(f"Modified URL: {new_url}")
            app.logger.warning(f"Original Currency: {original_currency}")
            app.logger.warning(f"Price extraction result: {prices}")
            return jsonify({
                'error': 'ì˜ëª»ëœ ë§í¬ë¥¼ ì…ë ¥í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤\nì‚¬ìš©ë²•ì„ í™•ì¸í•´ ì£¼ì„¸ìš”',
                'error_type': 'invalid_link',
                'step': step,
                'debug_info': {
                    'original_url': url,
                    'modified_url': new_url,
                    'original_currency': original_currency
                }
            }), 400
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
        download_filename = f"page_text_cid_{current_cid}.txt"
        download_link = f"/download/{download_filename}"
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            'step': step + 1,
            'total_steps': len(all_cids),
            'cid': current_cid,
            'cid_name': current_name,
            'url': new_url,
            'prices': prices,
            'found_count': len(prices),
            'process_time': round(process_time, 1),
            'has_next': step + 1 < len(all_cids),
            'next_step': step + 1 if step + 1 < len(all_cids) else None,
            'is_search_phase': is_search_phase,
            'phase_name': phase_name,
            'search_phase_completed': step + 1 == len(search_cids),
            'download_link': download_link,
            'download_filename': download_filename
        }
        
        return jsonify(result)
        
    except Exception as e:
        app.logger.error(f"Error in scraping: {str(e)}")
        return jsonify({
            'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'
        }), 500

@app.route('/guide')
def guide():
    """ì‚¬ìš©ë°©ë²• ê°€ì´ë“œ í˜ì´ì§€"""
    lang = request.args.get('lang', 'ko')  # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´
    return render_template('guide.html', lang=lang)

@app.route('/debug-files')
def debug_files():
    """ê°€ìƒì„œë²„ì—ì„œ íŒŒì¼ ì‹œìŠ¤í…œ ë””ë²„ê·¸"""
    import os
    import time
    import platform
    
    debug_info = []
    debug_info.append("ğŸ” ê°€ìƒì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œ ë””ë²„ê·¸")
    debug_info.append("="*50)
    
    # 1. ê¸°ë³¸ ì •ë³´
    debug_info.append(f"ğŸ“ í˜„ì¬ ìœ„ì¹˜: {os.getcwd()}")
    debug_info.append(f"ğŸ–¥ï¸  ìš´ì˜ì²´ì œ: {platform.system()} {platform.release()}")
    debug_info.append(f"âœï¸  ì“°ê¸° ê¶Œí•œ: {os.access('.', os.W_OK)}")
    
    # 2. downloads í´ë” í™•ì¸
    downloads_dir = 'downloads'
    debug_info.append(f"\nğŸ“ downloads í´ë”:")
    
    if not os.path.exists(downloads_dir):
        try:
            os.makedirs(downloads_dir)
            debug_info.append("   âœ… ìƒˆë¡œ ìƒì„±ë¨")
        except Exception as e:
            debug_info.append(f"   âŒ ìƒì„± ì‹¤íŒ¨: {e}")
    else:
        debug_info.append("   âœ… ì´ë¯¸ ì¡´ì¬")
    
    debug_info.append(f"   ì“°ê¸° ê¶Œí•œ: {os.access(downloads_dir, os.W_OK)}")
    
    # 3. ê°„ë‹¨í•œ íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸
    test_file = os.path.join(downloads_dir, f"web_debug_{int(time.time())}.txt")
    debug_info.append(f"\nâœï¸  íŒŒì¼ ì“°ê¸° í…ŒìŠ¤íŠ¸:")
    
    try:
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(f"ì›¹ ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸\nìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        if os.path.exists(test_file):
            size = os.path.getsize(test_file)
            debug_info.append(f"   âœ… ì„±ê³µ ({size} bytes)")
            
            # ë‚´ìš© ì½ê¸°
            with open(test_file, 'r', encoding='utf-8') as f:
                content = f.read()
                debug_info.append(f"   ğŸ“– ë‚´ìš©: {content[:50]}...")
            
        else:
            debug_info.append("   âŒ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            
    except Exception as e:
        debug_info.append(f"   âŒ ì˜¤ë¥˜: {e}")
    
    # 4. ì•„ê³ ë‹¤ ë°©ì‹ íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸
    debug_info.append(f"\nğŸ·ï¸  ì•„ê³ ë‹¤ ë°©ì‹ í…ŒìŠ¤íŠ¸:")
    agoda_file = os.path.join(downloads_dir, f"page_text_cid_WEBTEST.txt")
    
    try:
        # 1ë‹¨ê³„: ê¸°ë³¸ ìƒì„±
        with open(agoda_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("ğŸ” AGODA MAGIC PRICE - ì›¹ ë””ë²„ê·¸\n")
            f.write("="*80 + "\n")
            f.write(f"ğŸ“… ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        debug_info.append("   âœ… 1ë‹¨ê³„ ì„±ê³µ")
        
        # 2ë‹¨ê³„: ë‚´ìš© ì¶”ê°€
        with open(agoda_file, 'a', encoding='utf-8') as f:
            f.write(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°\n")
            f.write(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ\n")
            
        debug_info.append("   âœ… 2ë‹¨ê³„ ì„±ê³µ")
        
        # 3ë‹¨ê³„: ëŒ€ëŸ‰ í…ìŠ¤íŠ¸
        with open(agoda_file, 'a', encoding='utf-8') as f:
            test_content = "ì‹œì‘ê°€ â‚©64,039 í•œê¸€ í…ŒìŠ¤íŠ¸ " * 50
            f.write("="*80 + "\n")
            f.write(test_content)
        
        if os.path.exists(agoda_file):
            size = os.path.getsize(agoda_file)
            debug_info.append(f"   âœ… 3ë‹¨ê³„ ì„±ê³µ ({size:,} bytes)")
        else:
            debug_info.append("   âŒ 3ë‹¨ê³„ ì‹¤íŒ¨")
            
    except Exception as e:
        debug_info.append(f"   âŒ ì•„ê³ ë‹¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 5. ê¸°ì¡´ íŒŒì¼ ëª©ë¡
    debug_info.append(f"\nğŸ“‚ downloads í´ë” ë‚´ìš©:")
    try:
        files = os.listdir(downloads_dir)
        if files:
            for i, filename in enumerate(files[:10]):  # ìµœëŒ€ 10ê°œë§Œ
                file_path = os.path.join(downloads_dir, filename)
                file_size = os.path.getsize(file_path)
                debug_info.append(f"   {i+1}. {filename} ({file_size:,} bytes)")
            
            if len(files) > 10:
                debug_info.append(f"   ... ì´ {len(files)}ê°œ íŒŒì¼")
        else:
            debug_info.append("   (íŒŒì¼ ì—†ìŒ)")
                
    except Exception as e:
        debug_info.append(f"   âŒ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    debug_info.append(f"\nğŸ¯ ê²°ë¡ :")
    debug_info.append(f"   íŒŒì¼ ìƒì„±ì´ ì •ìƒ ì‘ë™í•œë‹¤ë©´ ìŠ¤í¬ë˜í¼ ë¬¸ì œì…ë‹ˆë‹¤.")
    debug_info.append(f"   íŒŒì¼ ìƒì„±ì´ ì‹¤íŒ¨í•œë‹¤ë©´ ì„œë²„ í™˜ê²½ ë¬¸ì œì…ë‹ˆë‹¤.")
    
    return "<pre style='font-family: monospace; background: #f5f5f5; padding: 20px; margin: 20px;'>" + "\n".join(debug_info) + "</pre>"

@app.route('/download/<filename>')
def download_file(filename):
    """í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        import os
        file_path = os.path.join('downloads', filename)
        
        if not os.path.exists(file_path):
            return jsonify({'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 404
        
        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='text/plain; charset=utf-8'
        )
        
    except Exception as e:
        app.logger.error(f"Error downloading file: {str(e)}")
        return jsonify({'error': f'ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}'}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)